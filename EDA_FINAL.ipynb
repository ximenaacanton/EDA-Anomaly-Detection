{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) on Large-Scale Operational Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains an exploratory data analysis (EDA) of a dataset related to operational processes, focusing on data cleaning, anomaly detection, and statistical insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------READ DATA-------------\n",
    "parquet_folder = \"/Users/Documents/data\"\n",
    "parquet_file = [\"DataTEC-11-01-to-11-30\", \"DataTEC-01-01-to-01-31\"]\n",
    "file_path = f\"{parquet_folder}/{parquet_file[0]}\"  # Nov\n",
    "file_path2 = f\"{parquet_folder}/{parquet_file[1]}\"  # Jan\n",
    "\n",
    "df = pq.read_table(file_path).to_pandas()\n",
    "df2 = pq.read_table(file_path2).to_pandas()\n",
    "\n",
    "print(\"The dataframe has: \", df.shape[0], \"observations\")\n",
    "print(\"The dataframe has: \", df2.shape[0], \"observations\")\n",
    "\n",
    "df_total = pd.concat([df, df2], ignore_index=True)\n",
    "\n",
    "df_total.head(5)  \n",
    "df_total.tail(5)  \n",
    "\n",
    "print(\"The dataframe has: \", df_total.shape[0], \"observations\")\n",
    "\n",
    "# Filtered variables list\n",
    "variables_filtradas = [\n",
    "    # Energy\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.EnergyState.0\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.EnergyMeasurement_kWh_ElectPower_MainMachine.0\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.ElectricalData_UPSPower.0\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.ElectricalData_MainMachine.0\",\n",
    "\n",
    "    # Preform temperature in different layers\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.PreformTemperatureLayer.1\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.PreformTemperatureLayer.3\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.PreformTemperatureLayer.5\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.PreformTemperatureLayer.7\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.PreformTemperatureLayer.9\",\n",
    "\n",
    "    # Other relevant temperatures\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.CurrentTemperatureRotaryJoint.0\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.CurrentPreformNeckFinishTemperature.0\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.CurrentTemperatureBrake.1\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.CurrentTemperatureBrake.2\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.CurrentPreformTemperatureOvenInfeed.0\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.ActualTemperatureCoolingCircuit2.0\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.CurrentTemperaturePressureDewPoint.0\",\n",
    "\n",
    "    # Cooling and air control\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.CoolingAirTemperatureActualValue.0\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.ContollerFactorCoolingCircuit1.0\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.AirWizardBasicController.0\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.AirWizardPlusController.0\",\n",
    "\n",
    "    # Pressure\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.PressureCompensationChamberPressureActualValue.0\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.FinalBlowingPressureActualValue.0\",\n",
    "\n",
    "    # Speed\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.WS_Cur_Mach_Spd.0\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.BeltDriveSpeedSetPoint.0\",\n",
    "\n",
    "    # Production and rejections\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.WS_Tot_Rej.0\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.WS_Tot_Bottles.0\",\n",
    "\n",
    "    # Other operational parameters\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.ActualHeightBaseCooling.0\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.CurrentProcessType_ConfigValue.0\"\n",
    "]\n",
    "\n",
    "df_vars = df_total[df_total[\"variable\"].isin(variables_filtradas)].copy()\n",
    "\n",
    "print(\"The dataframe has: \", df_vars.shape[0], \"observations\")\n",
    "\n",
    "# Percentage of the total observations represented by filtered data\n",
    "df_vars.shape[0] / df_total.shape[0]\n",
    "\n",
    "print(\"is a sample of \", round((df_vars.shape[0] / df_total.shape[0]) * 100), \"% of the observations \")\n",
    "\n",
    "#--------------------subtracting hours-------------------\n",
    "# Convert 'user_ts' to datetime \n",
    "df_vars[\"user_ts\"] = pd.to_datetime(df_vars[\"user_ts\"])\n",
    "# Subtract 6 hours\n",
    "df_vars[\"user_ts\"] = df_vars[\"user_ts\"] - pd.Timedelta(hours=6)\n",
    "df_vars.head(20)\n",
    "df_vars.tail(20)\n",
    "\n",
    "#----------------------MAKE THE DFs--------------------------\n",
    "# Create a dictionary of dataframes for each variable\n",
    "filtered_dfs = {var.replace(\".\", \"_\"): df_vars[df_vars[\"variable\"] == var].copy() for var in variables_filtradas}\n",
    "# Save the dataframes in individual variables\n",
    "df_names = {}\n",
    "for var, df_var in filtered_dfs.items():\n",
    "    globals()[f\"df_{var}\"] = df_var\n",
    "    df_names[f\"df_{var}\"] = df_var\n",
    "\n",
    "print(\"DataFrames created:\")\n",
    "for name in df_names.keys():\n",
    "    print(name)\n",
    "\n",
    "# NUM OF OBSERVATIONS PER VARIABLE IN DF TOTAL \n",
    "df_observaciones = {name: df.shape[0] for name, df in filtered_dfs.items()}\n",
    "df_observaciones_ordenado = dict(sorted(df_observaciones.items(), key=lambda item: item[1]))\n",
    "df_observaciones_ordenado\n",
    "\n",
    "# PERCENTAGE PER VARIABLE IN DF TOTAL \n",
    "proporcion_datos = {name: (df.shape[0] / df_total.shape[0]) * 100 for name, df in filtered_dfs.items()}\n",
    "df_proporcion_ordenado = dict(sorted(proporcion_datos.items(), key=lambda item: item[1]))\n",
    "df_proporcion_ordenado\n",
    "\n",
    "# Variables to be discarded\n",
    "variables_desechables = [ \n",
    "    'CONTIFORM_MMA_CONTIFORM_MMA1_CurrentProcessType_ConfigValue_0',\n",
    "    'CONTIFORM_MMA_CONTIFORM_MMA1_ContollerFactorCoolingCircuit1_0',\n",
    "    'CONTIFORM_MMA_CONTIFORM_MMA1_BeltDriveSpeedSetPoint_0',\n",
    "    'CONTIFORM_MMA_CONTIFORM_MMA1_ActualHeightBaseCooling_0',\n",
    "    'CONTIFORM_MMA_CONTIFORM_MMA1_WS_Cur_Mach_Spd_0',\n",
    "    'CONTIFORM_MMA_CONTIFORM_MMA1_EnergyState_0',\n",
    "    'CONTIFORM_MMA_CONTIFORM_MMA1_CoolingAirTemperatureActualValue_0',\n",
    "    'CONTIFORM_MMA_CONTIFORM_MMA1_WS_Tot_Rej_0'\n",
    "]\n",
    "\n",
    "# ELIMINATE NON REPRESENTATIVE VARIABLES < 0.1%\n",
    "for var in variables_desechables:\n",
    "    filtered_dfs.pop(var, None)\n",
    "\n",
    "print(\"Variables in filtered_dfs:\")\n",
    "for name in filtered_dfs.keys():\n",
    "    print(name)\n",
    "\n",
    "# ------------------- CREATE CHUNKS OF THE DFs ------------------\n",
    "def chunk_dataframe(df, chunk_size=100000):\n",
    "    \"\"\"Divide DataFrame into smaller chunks.\"\"\"\n",
    "    num_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size != 0 else 0)\n",
    "    return [df.iloc[i * chunk_size:(i + 1) * chunk_size] for i in range(num_chunks)]\n",
    "\n",
    "# Chunks for each df\n",
    "chunked_dfs = {name: chunk_dataframe(df_var) for name, df_var in filtered_dfs.items()}\n",
    "\n",
    "print(\"Chunks created per df:\")\n",
    "for name, chunks in chunked_dfs.items():\n",
    "    print(f\"{name}: {len(chunks)} chunks\")\n",
    "\n",
    "# ------------------- FUNCTIONS TO DETECT RELEVANCE AND RELATE ------------------\n",
    "def calculate_variability(df, column):\n",
    "    std_dev = df[column].std()\n",
    "    iqr = df[column].quantile(0.75) - df[column].quantile(0.25)\n",
    "    return {\"Standard Deviation\": std_dev, \"IQR\": iqr}\n",
    "\n",
    "def detect_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return len(outliers)\n",
    "\n",
    "def calculate_data_proportion(df, column):\n",
    "    return df[column].count() / df_total.shape[0]\n",
    "\n",
    "def decompress_message_first_chunk(chunked_dfs):\n",
    "    for name, chunks in chunked_dfs.items():\n",
    "        if chunks and \"message\" in chunks[0].columns:  # Check if chunks exist and 'message' is in the first one\n",
    "            chunk = chunks[0]\n",
    "\n",
    "            # Filter non-null 'message' values\n",
    "            if chunk[\"message\"].notna().any():\n",
    "                chunk = chunk.copy()  \n",
    "\n",
    "                # Convert to JSON if it's a string\n",
    "                chunk[\"message\"] = chunk[\"message\"].apply(lambda x: json.loads(x) if isinstance(x, str) else {})\n",
    "\n",
    "                # Extract data\n",
    "                extracted_columns = set()\n",
    "                for message in chunk[\"message\"]:\n",
    "                    if isinstance(message, dict):  \n",
    "                        extracted_columns.update(message.keys())\n",
    "\n",
    "                # Create new columns and populate them with JSON values\n",
    "                for col in extracted_columns:\n",
    "                    chunk[col] = chunk[\"message\"].apply(lambda x: x.get(col, None) if isinstance(x, dict) else None)\n",
    "\n",
    "                # Drop the 'message' column after extracting information\n",
    "                chunk.drop(columns=[\"message\"], inplace=True)\n",
    "\n",
    "                # Save the updated chunk in the respective dictionary\n",
    "                chunked_dfs[name] = chunk\n",
    "\n",
    "    return chunked_dfs\n",
    "\n",
    "chunked_dfs = decompress_message_first_chunk(chunked_dfs)\n",
    "\n",
    "print(\"Decompressed message data for relevant chunks.\")\n",
    "\n",
    "# Check the first 5 records of each first chunk in chunked_dfs\n",
    "for name, chunks in chunked_dfs.items():\n",
    "    if chunks:  # Check if there is at least one chunk available\n",
    "        print(f\"First chunk of {name}:\")\n",
    "        display(chunks[0].head(10))  # Show the first rows of the first chunk\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "#---------------APPLY FUNCTIONS TO THE CHUNK-----------------\n",
    "# Function to apply to all chunks\n",
    "def evaluate_all_chunks(chunked_dfs):\n",
    "    \"\"\"Applies relevant functions to all columns of the first chunks of each variable.\"\"\"\n",
    "    results = {}\n",
    "    for name, chunks in chunked_dfs.items():\n",
    "        if chunks:  # Check if there is at least one chunk available\n",
    "            chunk = chunks[0]  # Take the first chunk\n",
    "            results[name] = {}\n",
    "            for column in chunk.columns:\n",
    "                if chunk[column].dtype in [np.float64, np.int64]:  # Only analyze numeric variables\n",
    "                    var_result = calculate_variability(chunk, column)\n",
    "                    num_outliers = detect_outliers(chunk, column)\n",
    "                    data_proportion = calculate_data_proportion(chunk, column)\n",
    "                    results[name][column] = {\n",
    "                        \"Standard Deviation\": var_result[\"Standard Deviation\"],\n",
    "                        \"IQR\": var_result[\"IQR\"],\n",
    "                        \"Number of Outliers\": num_outliers,\n",
    "                        \"Data Proportion\": data_proportion\n",
    "                    }\n",
    "    return results\n",
    "\n",
    "relevance_results = evaluate_all_chunks(chunked_dfs)\n",
    "import pprint\n",
    "pprint.pprint(relevance_results)\n",
    "\n",
    "#--------------AFTER ANALYZING METRICS---------------------\n",
    "\n",
    "# LOW VARIABILITY\n",
    "low_variability_variables = [\n",
    "    \"CONTIFORM_MMA_CONTIFORM_MMA1_AirWizardBasicController_0\", \n",
    "    \"CONTIFORM_MMA_CONTIFORM_MMA1_AirWizardPlusController_0\",\n",
    "    \"CONTIFORM_MMA_CONTIFORM_MMA1_ElectricalData_MainMachine_0.voltageMaxL1\",\n",
    "    \"CONTIFORM_MMA_CONTIFORM_MMA1_ElectricalData_UPSPower_0\"\n",
    "]\n",
    "\n",
    "# PLOT INDIVIDUAL GRAPHS ONE BY ONE\n",
    "for var in low_variability_variables:\n",
    "    for name, chunks in chunked_dfs.items():\n",
    "        if name == var and chunks:\n",
    "            chunk = chunks[0]  # First chunk\n",
    "            for column in chunk.columns:\n",
    "                if column not in [\"user_ts\", \"variable\"]: \n",
    "                    plt.figure(figsize=(10, 5))\n",
    "                    if \"user_ts\" in chunk.columns:\n",
    "                        plt.plot(chunk[\"user_ts\"], chunk[column], marker='o', linestyle='-', alpha=0.5)\n",
    "                        plt.xlabel(\"Time (user_ts)\")\n",
    "                    else:\n",
    "                        plt.plot(chunk.index, chunk[column], marker='o', linestyle='-', alpha=0.5)\n",
    "                        plt.xlabel(\"Index\")\n",
    "                    plt.title(f\"Evolution of {column} in {var} (first chunk)\")\n",
    "                    plt.ylabel(column)\n",
    "                    plt.xticks(rotation=45)\n",
    "                    plt.grid(True)\n",
    "                    plt.show()\n",
    "\n",
    "# GROUPED PLOTS \n",
    "for var in low_variability_variables:\n",
    "    for name, chunks in chunked_dfs.items():\n",
    "        if name == var and chunks:\n",
    "            chunk = chunks[0]  # First chunk\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            for column in chunk.columns:\n",
    "                if column not in [\"user_ts\", \"variable\"]: \n",
    "                    if \"user_ts\" in chunk.columns:\n",
    "                        plt.plot(chunk[\"user_ts\"], chunk[column], marker='o', linestyle='-', alpha=0.5, label=column)\n",
    "                    else:\n",
    "                        plt.plot(chunk.index, chunk[column], marker='o', linestyle='-', alpha=0.5, label=column)\n",
    "            plt.xlabel(\"Time (user_ts)\" if \"user_ts\" in chunk.columns else \"Index\")\n",
    "            plt.title(f\"Evolution of all columns in {var} (first chunk)\")\n",
    "            plt.ylabel(\"Value\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "#--------------------DF TO CSV OF FINAL VARIABLES---------------------\n",
    "'''\n",
    "variables_to_export = [\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.PreformTemperatureLayer.1\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.PreformTemperatureLayer.3\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.PreformTemperatureLayer.5\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.PreformTemperatureLayer.7\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.PreformTemperatureLayer.9\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.FinalBlowingPressureActualValue.0\",\n",
    "    \"CONTIFORM_MMA.CONTIFORM_MMA1.PressureCompensationChamberPressureActualValue.0\"\n",
    "]\n",
    "\n",
    "observations_per_df = {var: filtered_dfs[var.replace(\".\", \"_\")].shape[0] for var in variables_to_export if var.replace(\".\", \"_\") in filtered_dfs}\n",
    "\n",
    "# Display the result\n",
    "for var, num_obs in observations_per_df.items():\n",
    "    print(f\"{var}: {num_obs} observations\")\n",
    "\n",
    "for var in variables_to_export:\n",
    "    formatted_var = var.replace(\".\", \"_\")  # Replace dots with underscores\n",
    "    if formatted_var in filtered_dfs:  # Check if it exists in the DataFrames\n",
    "        file_name = f\"{formatted_var}.csv\"\n",
    "        filtered_dfs[formatted_var].to_csv(file_name, index=False)\n",
    "        print(f\"Saved: {file_name}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "data_pressure_soplado = df_CONTIFORM_MMA_CONTIFORM_MMA1_FinalBlowingPressureActualValue_0.copy()\n",
    "data_pressure_compensado = df_CONTIFORM_MMA_CONTIFORM_MMA1_PressureCompensationChamberPressureActualValue_0.copy()\n",
    "\n",
    "#-------------------------------PRESSURE SOPLADO -------------------------\n",
    "# ---- INITIAL DESCRIPTIVE ANALYSIS ----\n",
    "print(\"Dataset information:\")\n",
    "print(data_pressure_soplado.info())\n",
    "print(\"\\nNull values:\")\n",
    "print(data_pressure_soplado.isnull().sum())\n",
    "print(\"\\nDuplicate values:\")\n",
    "print(data_pressure_soplado.duplicated().sum())\n",
    "# data_pressure_soplado.drop(columns='message', inplace=True)\n",
    "# STADISTIC SUMMARY\n",
    "print(\"\\nStatistical summary:\")\n",
    "print(data_pressure_soplado.describe(percentiles=[0.25, 0.5, 0.75]))\n",
    "data_pressure_soplado.shape[0]\n",
    "data_pressure_soplado.head(20)\n",
    "data_pressure_compensado.shape[0]\n",
    "\n",
    "# ---- DISTRIBUTIONS AND OUTLIERS DETECTION ----\n",
    "\n",
    "col = 'value'\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.figure()\n",
    "sns.histplot(data_pressure_soplado[col], kde=True, bins=20)\n",
    "plt.title(f\"Final Blowing Pressure Distribution\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(y=data_pressure_soplado[col])\n",
    "plt.title(f\"Final Blowing Pressure Boxplot\")\n",
    "plt.show()\n",
    "print(col)\n",
    "\n",
    "# IQR FOR OUTLIERS\n",
    "\n",
    "Q1 = data_pressure_soplado[col].quantile(0.25)\n",
    "Q3 = data_pressure_soplado[col].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = data_pressure_soplado[(data_pressure_soplado[col] < (Q1 - 1.5 * IQR)) | (data_pressure_soplado[col] > (Q3 + 1.5 * IQR))]\n",
    "print(f\"Outliers detected in {col} by IQR:\")\n",
    "print(outliers[[col]])\n",
    "print(col)\n",
    "\n",
    "# ---- TEMPORAL ANALYSIS AND TENDENCIES ----\n",
    "data_pressure_soplado[\"user_ts\"] = pd.to_datetime(data_pressure_soplado[\"user_ts\"])\n",
    "data_pressure_soplado = data_pressure_soplado.sort_values(by=\"user_ts\")\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(data_pressure_soplado[\"user_ts\"], data_pressure_soplado[col], label=col)\n",
    "plt.title(f\"Time Series of {col}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(data_pressure_soplado[\"user_ts\"], data_pressure_soplado[col].rolling(window=30).mean(), label=\"Rolling Mean\")\n",
    "plt.plot(data_pressure_soplado[\"user_ts\"], data_pressure_soplado[col].rolling(window=30).std(), label=\"Rolling Std\")\n",
    "plt.title(f\"Rolling Mean & Std of {col} FinalBlowingPressureActualValue\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#-------------------------PRESSURE COMPENSADO--------------------\n",
    "# --- INITIAL DESCRIPTIVE ANALYSIS ----\n",
    "print(\"Dataset information:\")\n",
    "print(data_pressure_compensado.info())\n",
    "print(\"\\nNull values:\")\n",
    "print(data_pressure_compensado.isnull().sum())\n",
    "print(\"\\nDuplicate values:\")\n",
    "print(data_pressure_compensado.duplicated().sum())\n",
    "# data_pressure_soplado.drop(columns='message', inplace=True)\n",
    "# STATISTIC SUMMARY\n",
    "print(\"\\nStatistical summary:\")\n",
    "print(data_pressure_compensado.describe(percentiles=[0.25, 0.5, 0.75]))\n",
    "\n",
    "# ----- DISTRIBUTIONS AND OUTLIERS DETECTION ----\n",
    "\n",
    "col = 'value'\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.figure()\n",
    "sns.histplot(data_pressure_compensado[col], kde=True, bins=10)\n",
    "plt.title(f\"Pressure Chamber Compensation Distribution:\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.boxplot(y=data_pressure_compensado[col])\n",
    "plt.title(f\"Pressure Chamber Compensation Boxplot:\")\n",
    "plt.show()\n",
    "print(col)\n",
    "\n",
    "# IQR FOR OUTLIERS\n",
    "Q1 = data_pressure_compensado[col].quantile(0.25)\n",
    "Q3 = data_pressure_compensado[col].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = data_pressure_compensado[(data_pressure_compensado[col] < (Q1 - 1.5 * IQR)) | (data_pressure_soplado[col] > (Q3 + 1.5 * IQR))]\n",
    "print(f\"Outliers detected in {col} by IQR:\")\n",
    "print(outliers[[col]])\n",
    "print(col)\n",
    "\n",
    "# ---- TEMPORAL ANALYSIS AND TENDENCIES ----\n",
    "data_pressure_compensado[\"user_ts\"] = pd.to_datetime(data_pressure_compensado[\"user_ts\"])\n",
    "data_pressure_compensado = data_pressure_compensado.sort_values(by=\"user_ts\")\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(data_pressure_compensado[\"user_ts\"], data_pressure_compensado[col], label=col)\n",
    "plt.title(f\"Time Series of {col}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(data_pressure_compensado[\"user_ts\"], data_pressure_compensado[col].rolling(window=30).mean(), label=\"Rolling Mean\")\n",
    "plt.plot(data_pressure_compensado[\"user_ts\"], data_pressure_compensado[col].rolling(window=30).std(), label=\"Rolling Std\")\n",
    "plt.title(f\"Rolling Mean & Std of {col} FinalBlowingPressureActualValue\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# TIME SERIES FILTERED\n",
    "# PRESSURE CHAMBER\n",
    "\n",
    "data_pressure_compensado[\"user_ts\"] = pd.to_datetime(data_pressure_compensado[\"user_ts\"])\n",
    "# NOV 1- JAN 2\n",
    "start_date = \"2024-11-15\"\n",
    "end_date = \"2025-12-01\"\n",
    "filtered_data = data_pressure_compensado[(data_pressure_compensado[\"user_ts\"] >= start_date) & \n",
    "                                         (data_pressure_compensado[\"user_ts\"] <= end_date)]\n",
    "\n",
    "filtered_data = filtered_data.sort_values(by=\"user_ts\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(filtered_data[\"user_ts\"], filtered_data[col], label=col)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Pressure Value\")\n",
    "plt.title(f\"Time Series of Pressure in Compensation Chamber (Nov 15 - Dec 2)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# PRESSURE SOPLADO\n",
    "data_pressure_soplado[\"user_ts\"] = pd.to_datetime(data_pressure_soplado[\"user_ts\"])\n",
    "start_date = \"2024-11-15\"\n",
    "end_date = \"2025-12-01\"\n",
    "filtered_data = data_pressure_soplado[(data_pressure_soplado[\"user_ts\"] >= start_date) & \n",
    "                                         (data_pressure_soplado[\"user_ts\"] <= end_date)]\n",
    "\n",
    "filtered_data = filtered_data.sort_values(by=\"user_ts\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(filtered_data[\"user_ts\"], filtered_data[col], label=col)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Pressure Value\")\n",
    "plt.title(f\"Time Series of Blowing Pressure (Nov 11 - Dec 2)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------CONTIFORM_MMA_CONTIFORM_MMA1_WS_Tot_Rej_0.csv------------------\n",
    "\n",
    "# ---- 1.1. Initial Descriptive Analysis ----\n",
    "print(\"Dataset information:\")\n",
    "print(data.info())\n",
    "print(\"\\nNull values:\")\n",
    "print(data.isnull().sum())\n",
    "print(\"\\nDuplicate values:\")\n",
    "print(data.duplicated().sum())\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nStatistical summary:\")\n",
    "print(data.describe(percentiles=[0.25, 0.5, 0.75]))\n",
    "\n",
    "# ---- 1.2. Distribution Analysis and Outlier Detection ----\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for col in data.select_dtypes(include=np.number).columns:\n",
    "    col_data = data[col].dropna()  # Remove NaN values\n",
    "    \n",
    "    if col_data.nunique() > 1:  # Ensure there is more than one unique value\n",
    "        # Histogram and KDE\n",
    "        plt.figure()\n",
    "        sns.histplot(col_data, kde=True, bins=30)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Boxplot\n",
    "        plt.figure()\n",
    "        sns.boxplot(y=col_data)\n",
    "        plt.title(f\"Boxplot of {col}\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"The column '{col}' has insufficient values for plotting.\")\n",
    "\n",
    "    # Z-score to detect outliers\n",
    "    z_scores = np.abs(zscore(col_data))\n",
    "    outliers_z = col_data[z_scores > 3]\n",
    "    \n",
    "    print(f\"\\nOutliers detected in '{col}' (Z-score > 3):\")\n",
    "    if not outliers_z.empty:\n",
    "        print(outliers_z)\n",
    "    else:\n",
    "        print(\"No outliers found with Z-score.\")\n",
    "\n",
    "# IQR (Interquartile Range method) to detect outliers\n",
    "for col in data.select_dtypes(include=np.number).columns:\n",
    "    col_data = data[col].dropna()  # Remove NaN values\n",
    "\n",
    "    Q1 = col_data.quantile(0.25)\n",
    "    Q3 = col_data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    outliers_iqr = col_data[(col_data < (Q1 - 1.5 * IQR)) | (col_data > (Q3 + 1.5 * IQR))]\n",
    "\n",
    "    print(f\"\\nOutliers detected in '{col}' using IQR:\")\n",
    "    if not outliers_iqr.empty:\n",
    "        print(outliers_iqr)\n",
    "    else:\n",
    "        print(\"No outliers found with IQR.\")\n",
    "\n",
    "\n",
    "# ---- 1.3. Temporal Analysis and Trends ----\n",
    "if \"user_ts\" in data.columns:\n",
    "    print(\"\\nThe 'user_ts' column exists. Converting to datetime format...\\n\")\n",
    "    \n",
    "    data[\"user_ts\"] = pd.to_datetime(data[\"user_ts\"], errors=\"coerce\")  # Error handling\n",
    "    data = data.dropna(subset=[\"user_ts\"])  # Remove NaN values in dates\n",
    "    data = data.sort_values(by=\"user_ts\")\n",
    "\n",
    "    print(data.info())  # Debugging: check if conversion was successful\n",
    "    \n",
    "    # Plotting time series\n",
    "    for col in data.select_dtypes(include=np.number).columns:\n",
    "        if data[col].dropna().empty:  # Avoid plotting columns with no valid data\n",
    "            print(f\"The column '{col}' has no valid numerical data.\")\n",
    "            continue\n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(data[\"user_ts\"], data[col], label=col)\n",
    "        plt.title(f\"Time series of {col}\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(col)\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show(block=True)  # Ensure the plot is shown\n",
    "\n",
    "    # Rolling Mean & Std\n",
    "    for col in data.select_dtypes(include=np.number).columns:\n",
    "        if len(data[col].dropna()) < 30:  # Adjust window size if there is little data\n",
    "            window_size = max(1, len(data[col].dropna()) // 2)  # Adjust to half if there is little data\n",
    "            print(f\"Adjusting rolling window to {window_size} for '{col}' (insufficient data).\")\n",
    "        else:\n",
    "            window_size = 30\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(data[\"user_ts\"], data[col].rolling(window=window_size).mean(), label=\"Rolling Mean\", color=\"blue\")\n",
    "        plt.plot(data[\"user_ts\"], data[col].rolling(window=window_size).std(), label=\"Rolling Std\", color=\"red\")\n",
    "        plt.title(f\"Rolling Mean & Std of {col}\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show(block=True)\n",
    "else:\n",
    "    print(\"\\nThe 'user_ts' column does not exist in the DataFrame.\\n\")\n",
    "# Calculate Q1, Q3, and IQR\n",
    "Q1 = data['value'].quantile(0.25)\n",
    "Q3 = data['value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the limits\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Check the limits\n",
    "print(f\"Lower bound: {lower_bound}\")\n",
    "print(f\"Upper bound: {upper_bound}\")\n",
    "\n",
    "# Count values near the limits\n",
    "print(f\"Values less than {lower_bound + 0.1 * IQR}: {sum(data['value'] < (lower_bound + 0.1 * IQR))}\")\n",
    "print(f\"Values greater than {upper_bound - 0.1 * IQR}: {sum(data['value'] > (upper_bound - 0.1 * IQR))}\")\n",
    "\n",
    "# Adjust the criteria if necessary\n",
    "lower_bound_adj = Q1 - 2.0 * IQR\n",
    "upper_bound_adj = Q3 + 2.0 * IQR\n",
    "\n",
    "anomalies = data[(data['value'] < lower_bound_adj) | (data['value'] > upper_bound_adj)]\n",
    "\n",
    "print(\"Detected anomalies (adjusted criteria):\")\n",
    "print(anomalies)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(data[\"user_ts\"], data[\"value\"], marker=\"o\", linestyle=\"-\")\n",
    "plt.title(\"Evolution of 'value' over time\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Specify the exact date and time to analyze\n",
    "start_datetime = \"2024-11-15 12:20:00\"\n",
    "end_datetime = \"2024-12-01 12:45:00\"\n",
    "\n",
    "# Filter data within that interval\n",
    "filtered_df = data[(data[\"user_ts\"] >= start_datetime) & (data[\"user_ts\"] <= end_datetime)]\n",
    "\n",
    "# Verify filtered data\n",
    "print(\"Filtered data in the selected interval:\")\n",
    "print(filtered_df.head())\n",
    "\n",
    "# Analyze basic statistics of behavior during that time\n",
    "print(\"\\nStatistical summary:\")\n",
    "print(filtered_df.describe())\n",
    "\n",
    "# Plot evolution of variables over time within that interval\n",
    "plt.figure(figsize=(10, 4))\n",
    "for col in filtered_df.select_dtypes(include=\"number\").columns:\n",
    "    plt.plot(filtered_df[\"user_ts\"], filtered_df[col], label=col)\n",
    "\n",
    "plt.title(f\"Behavior in the interval {start_datetime} - {end_datetime}\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "#-------------CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_7----------------\n",
    "\n",
    "# Load the CSV file\n",
    "file = \"/Users/Documents/CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_7.csv\"  \n",
    "df = pd.read_csv(file)\n",
    "\n",
    "# Convert the time column to datetime format\n",
    "df[\"user_ts\"] = pd.to_datetime(df[\"user_ts\"], format=\"ISO8601\", errors=\"coerce\")\n",
    "\n",
    "# Specify the exact date and time to analyze\n",
    "start_datetime = \"2024-11-01 12:20:00\"\n",
    "end_datetime = \"2025-02-01 12:45:00\"\n",
    "\n",
    "# Filter data within that interval\n",
    "filtered_df = df[(df[\"user_ts\"] >= start_datetime) & (df[\"user_ts\"] <= end_datetime)]\n",
    "\n",
    "# Verify filtered data\n",
    "print(\"Filtered data in the selected interval:\")\n",
    "print(filtered_df.head())\n",
    "\n",
    "# Analyze basic statistics of behavior during that time\n",
    "print(\"\\nStatistical summary:\")\n",
    "print(filtered_df.describe())\n",
    "\n",
    "# Plot evolution of variables over time within that interval\n",
    "plt.figure(figsize=(10, 4))\n",
    "for col in filtered_df.select_dtypes(include=\"number\").columns:\n",
    "    plt.plot(filtered_df[\"user_ts\"], filtered_df[col], label=col)\n",
    "\n",
    "plt.title(f\"Behavior in the interval {start_datetime} - {end_datetime}\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Calculate statistics for each numeric column\n",
    "statistics = df[numeric_columns].describe(percentiles=[0.25, 0.5, 0.75]).T\n",
    "statistics[\"std\"] = df[numeric_columns].std()  # Add standard deviation\n",
    "\n",
    "# Show results\n",
    "print(\"Statistical summary of numeric variables:\")\n",
    "print(statistics)\n",
    "\n",
    "# Total number of records\n",
    "total_records = df.shape[0]\n",
    "\n",
    "# Null values per column\n",
    "null_values = df.isnull().sum()\n",
    "\n",
    "# Duplicated values\n",
    "duplicated_values = df.duplicated().sum()\n",
    "\n",
    "# Show results\n",
    "print(f\"Total records: {total_records}\")\n",
    "print(f\"Null values per column:\\n{null_values}\")\n",
    "print(f\"Total duplicated values: {duplicated_values}\")\n",
    "\n",
    "# ---- 1.2. Distribution Analysis and Outlier Detection ----\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for col in df.select_dtypes(include=np.number).columns:\n",
    "    col_df = df[col].dropna()  # Remove NaN values\n",
    "    \n",
    "    if col_df.nunique() > 1:  # Ensure there is more than one unique value\n",
    "        # Histogram and KDE\n",
    "        plt.figure()\n",
    "        sns.histplot(col_df, kde=True, bins=30)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Boxplot\n",
    "        plt.figure()\n",
    "        sns.boxplot(y=col_df)\n",
    "        plt.title(f\"Boxplot of {col}\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"The column '{col}' has insufficient values for plotting.\")\n",
    "\n",
    "Q1 = filtered_df['value'].quantile(0.25)\n",
    "Q3 = filtered_df['value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier limits\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers\n",
    "anomalies = filtered_df[(filtered_df['value'] < lower_bound) | (filtered_df['value] > upper_bound)]\n",
    "\n",
    "print(\"Anomalies detected:\")\n",
    "print(anomalies)\n",
    "\n",
    "# IQR (Interquartile Range method) to detect outliers\n",
    "for col in df.select_dtypes(include=np.number).columns:\n",
    "    col_df = df[col].dropna()  # Remove NaN values\n",
    "\n",
    "    Q1 = col_df.quantile(0.25)\n",
    "    Q3 = col_df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    outliers_iqr = col_df[(col_df < (Q1 - 1.5 * IQR)) | (col_df > (Q3 + 1.5 * IQR))]\n",
    "\n",
    "    print(f\"\\nOutliers detected in '{col}' using IQR:\")\n",
    "    if not outliers_iqr.empty:\n",
    "        print(outliers_iqr)\n",
    "    else:\n",
    "        print(\"No outliers found using IQR.\")  # Specify the exact date and time for analysis\n",
    "\n",
    "start_datetime = \"2024-11-15 12:20:00\"\n",
    "end_datetime = \"2025-12-01 12:45:00\"\n",
    "\n",
    "# Filter data within that interval\n",
    "filtered_df = df[(df[\"user_ts\"] >= fecha_hora_inicio) & (df[\"user_ts\"] <= fecha_hora_fin)]\n",
    "\n",
    "# Verify filtered data\n",
    "print(\"Filtered data in the selected interval:\")\n",
    "print(filtered_df.head())\n",
    "\n",
    "# Analyze basic statistics of the behavior within that time\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(filtered_dfo.describe())\n",
    "\n",
    "# Plot evolution of variables over time within that interval\n",
    "plt.figure(figsize=(10, 4))\n",
    "for col in filtered_df.select_dtypes(include=\"number\").columns:\n",
    "    plt.plot(filtered_df[\"user_ts\"], filtered_df[col], label=col)\n",
    "\n",
    "plt.title(f\"Behavior in the interval {start_datetime} - {end_datetime}\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Rolling Mean & Std\n",
    "for col in df.select_dtypes(include=np.number).columns:\n",
    "        if len(df[col].dropna()) < 30:  # Adjust the window size if there are few data points\n",
    "            window_size = max(1, len(df[col].dropna()) // 2)  # Adjust to half if there are few data points\n",
    "            print(f\"Warning: Adjusting rolling window to {window_size} for '{col}' (few data).\")\n",
    "        else:\n",
    "            window_size = 30\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(df[\"user_ts\"], df[col].rolling(window=window_size).mean(), label=\"Rolling Mean\", color=\"blue\")\n",
    "        plt.plot(df[\"user_ts\"], df[col].rolling(window=window_size).std(), label=\"Rolling Std\", color=\"red\")\n",
    "        plt.title(f\"Rolling Mean & Std of {col}\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------Layer9.ipynb---------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Style configuration\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# CSV file path\n",
    "csv_file = \"CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_9.csv\"  # MODIFY THIS\n",
    "\n",
    "# Load the CSV\n",
    "data = pd.read_csv(csv_file)\n",
    "print(data)\n",
    "\n",
    "# ---- 1.1. Initial Descriptive Analysis ----\n",
    "print(\"Dataset Information:\")\n",
    "print(data.info())\n",
    "print(\"\\nNull Values:\")\n",
    "print(data.isnull().sum())\n",
    "print(\"\\nDuplicate Values:\")\n",
    "print(data.duplicated().sum())\n",
    "\n",
    "data.dropna(subset=['value'], inplace=True)\n",
    "data.drop(columns=['message'], inplace=True)\n",
    "\n",
    "data\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(data.describe(percentiles=[0.25, 0.5, 0.75]))\n",
    "\n",
    "# ---- 1.2. Distribution Analysis and Outlier Detection ----\n",
    "plt.figure(figsize=(10, 5))\n",
    "for col in data.select_dtypes(include=np.number).columns:\n",
    "    plt.figure()\n",
    "    sns.histplot(data[col], kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    sns.boxplot(y=data[col])\n",
    "    plt.title(f\"Boxplot of {col}\")\n",
    "    plt.show()\n",
    "\n",
    "# Z-score for outlier detection\n",
    "    data[f\"{col}_zscore\"] = np.abs(zscore(data[col]))\n",
    "    print(f\"Outliers detected in {col} (Z-score > 3):\")\n",
    "    print(data[data[f\"{col}_zscore\"] > 3][[col]])\n",
    "\n",
    "# IQR (Interquartile Range Method) for outlier detection\n",
    "for col in data.select_dtypes(include=np.number).columns:\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = data[(data[col] < (Q1 - 1.5 * IQR)) | (data[col] > (Q3 + 1.5 * IQR))]\n",
    "    print(f\"Outliers detected in {col} by IQR:\")\n",
    "    print(outliers[[col]])\n",
    "\n",
    "if \"user_ts\" in data.columns:\n",
    "    # The format string now matches the timestamp format in your data\n",
    "    data[\"user_ts\"] = pd.to_datetime(data[\"user_ts\"], format='%Y-%m-%d %H:%M:%S%z', errors='coerce')\n",
    "    data = data.dropna(subset=['user_ts'])\n",
    "    data = data.sort_values(by=\"user_ts\")\n",
    "    for col in data.select_dtypes(include=np.number).columns:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(data[\"user_ts\"], data[col], label=col)\n",
    "        plt.title(f\"Time Series of {col}\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Rolling Mean & Std\n",
    "    for col in data.select_dtypes(include=np.number).columns:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(data[\"user_ts\"], data[col].rolling(window=30).mean(), label=\"Rolling Mean\")\n",
    "        plt.plot(data[\"user_ts\"], data[col].rolling(window=30).std(), label=\"Rolling Std\")\n",
    "        plt.title(f\"Rolling Mean & Std of {col}\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# ---- 1.4. Model-Based Anomaly Detection ----\n",
    "X = data.select_dtypes(include=np.number)  # Only numeric variables\n",
    "if not X.empty:\n",
    "    X = X.dropna()\n",
    "    # Linear regression\n",
    "    if X.shape[1] > 1:\n",
    "        reg = LinearRegression()\n",
    "        reg.fit(X.iloc[:, :-1], X.iloc[:, -1])\n",
    "        print(\"Linear regression coefficients:\")\n",
    "        print(reg.coef_)\n",
    "\n",
    "    # DBSCAN for anomaly detection\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    data[\"dbscan_anomaly\"] = dbscan.fit_predict(X)\n",
    "\n",
    "    # Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "    data[\"isolation_anomaly\"] = iso_forest.fit_predict(X)\n",
    "\n",
    "    # LOF (Local Outlier Factor)\n",
    "    lof = LocalOutlierFactor(n_neighbors=20)\n",
    "    data[\"lof_anomaly\"] = lof.fit_predict(X)\n",
    "\n",
    "    # ---- Visualize anomalies ----\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.scatterplot(x=X.iloc[:, 0], y=X.iloc[:, 1] if X.shape[1] > 1 else X.iloc[:, 0], hue=data[\"isolation_anomaly\"], palette={1:\"blue\", -1:\"red\"})\n",
    "    plt.title(\"Anomalies detected with Isolation Forest\")\n",
    "    plt.legend(title=\"Anomaly\", labels=[\"Normal\", \"Anomalous\"])\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nNumber of anomalies detected by each method:\")\n",
    "    print(\"Isolation Forest:\")\n",
    "    print(data[\"isolation_anomaly\"].value_counts())\n",
    "    print(\"DBSCAN:\")\n",
    "    print(data[\"dbscan_anomaly\"].value_counts())\n",
    "    print(\"Local Outlier Factor (LOF):\")\n",
    "    print(data[\"lof_anomaly\"].value_counts())\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------Layer1.ipynb-------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Style configuration\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# CSV file path\n",
    "csv_file = \"CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_1.csv\"  # MODIFY THIS\n",
    "\n",
    "# Load the CSV\n",
    "data = pd.read_csv(csv_file)\n",
    "print(data)\n",
    "\n",
    "# ---- 1.1. Initial Descriptive Analysis ----\n",
    "print(\"Dataset Information:\")\n",
    "print(data.info())\n",
    "print(\"\\nNull Values:\")\n",
    "print(data.isnull().sum())\n",
    "print(\"\\nDuplicate Values:\")\n",
    "print(data.duplicated().sum())\n",
    "\n",
    "\"\"\"Remove null values from 'value' and remove the 'message' column as it contains only null values.\"\"\"\n",
    "\n",
    "data.dropna(subset=['value'], inplace=True)\n",
    "data.drop(columns=['message'], inplace=True)\n",
    "\n",
    "data\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(data.describe(percentiles=[0.25, 0.5, 0.75]))\n",
    "\n",
    "# ---- 1.2. Distribution Analysis and Outlier Detection ----\n",
    "plt.figure(figsize=(10, 5))\n",
    "for col in data.select_dtypes(include=np.number).columns:\n",
    "    plt.figure()\n",
    "    sns.histplot(data[col], kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    sns.boxplot(y=data[col])\n",
    "    plt.title(f\"Boxplot of {col}\")\n",
    "    plt.show()\n",
    "\n",
    "# Z-score for outlier detection\n",
    "    data[f\"{col}_zscore\"] = np.abs(zscore(data[col]))\n",
    "    print(f\"Outliers detected in {col} (Z-score > 3):\")\n",
    "    print(data[data[f\"{col}_zscore\"] > 3][[col]])\n",
    "\n",
    "# IQR (Interquartile Range Method) for outlier detection\n",
    "for col in data.select_dtypes(include=np.number).columns:\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = data[(data[col] < (Q1 - 1.5 * IQR)) | (data[col] > (Q3 + 1.5 * IQR))]\n",
    "    print(f\"Outliers detected in {col} by IQR:\")\n",
    "    print(outliers[[col]])\n",
    "\n",
    "if \"user_ts\" in data.columns:\n",
    "    # The format string now matches the timestamp format in your data\n",
    "    data[\"user_ts\"] = pd.to_datetime(data[\"user_ts\"], format='%Y-%m-%d %H:%M:%S%z', errors='coerce')\n",
    "    data = data.dropna(subset=['user_ts'])\n",
    "    data = data.sort_values(by=\"user_ts\")\n",
    "    for col in data.select_dtypes(include=np.number).columns:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(data[\"user_ts\"], data[col], label=col)\n",
    "        plt.title(f\"Time Series of {col}\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Rolling Mean & Std\n",
    "    for col in data.select_dtypes(include=np.number).columns:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(data[\"user_ts\"], data[col].rolling(window=30).mean(), label=\"Rolling Mean\")\n",
    "        plt.plot(data[\"user_ts\"], data[col].rolling(window=30).std(), label=\"Rolling Std\")\n",
    "        plt.title(f\"Rolling Mean & Std of {col}\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# ---- 1.4. Model-Based Anomaly Detection ----\n",
    "X = data.select_dtypes(include=np.number)  # Only numeric variables\n",
    "if not X.empty:\n",
    "    X = X.dropna()\n",
    "    # Linear regression\n",
    "    if X.shape[1] > 1:\n",
    "        reg = LinearRegression()\n",
    "        reg.fit(X.iloc[:, :-1], X.iloc[:, -1])\n",
    "        print(\"Linear regression coefficients:\")\n",
    "        print(reg.coef_)\n",
    "\n",
    "    # DBSCAN for anomaly detection\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    data[\"dbscan_anomaly\"] = dbscan.fit_predict(X)\n",
    "\n",
    "    # Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "    data[\"isolation_anomaly\"] = iso_forest.fit_predict(X)\n",
    "\n",
    "    # LOF (Local Outlier Factor)\n",
    "    lof = LocalOutlierFactor(n_neighbors=20)\n",
    "    data[\"lof_anomaly\"] = lof.fit_predict(X)\n",
    "\n",
    "    # ---- Visualize anomalies ----\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.scatterplot(x=X.iloc[:, 0], y=X.iloc[:, 1] if X.shape[1] > 1 else X.iloc[:, 0], hue=data[\"isolation_anomaly\"], palette={1:\"blue\", -1:\"red\"})\n",
    "    plt.title(\"Anomalies detected with Isolation Forest\")\n",
    "    plt.legend(title=\"Anomaly\", labels=[\"Normal\", \"Anomalous\"])\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nNumber of anomalies detected by each method:\")\n",
    "    print(\"Isolation Forest:\")\n",
    "    print(data[\"isolation_anomaly\"].value_counts())\n",
    "    print(\"DBSCAN:\")\n",
    "    print(data[\"dbscan_anomaly\"].value_counts())\n",
    "    print(\"Local Outlier Factor (LOF):\")\n",
    "    print(data[\"lof_anomaly\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_5 = pd.read_csv(\"PreformTemperatureLayer_5.csv\")\n",
    "layer_3 = pd.read_csv(\"PreformTemperatureLayer_3.csv\")\n",
    "\n",
    "# ---- 1.1. Initial Descriptive Analysis ----\n",
    "print(\"Dataset information for layer 3:\")\n",
    "print(layer_3.info())\n",
    "print(\"Dataset information for layer 5:\")\n",
    "print(layer_5.info())\n",
    "\n",
    "print(\"\\nNull values in layer 3:\")\n",
    "print(layer_3.isnull().sum())\n",
    "print(\"\\nNull values in layer 5:\")\n",
    "print(layer_5.isnull().sum())\n",
    "\n",
    "print(\"\\nDuplicate values in layer 3:\")\n",
    "print(layer_3.duplicated().sum())\n",
    "print(\"\\nDuplicate values in layer 5:\")\n",
    "print(layer_5.duplicated().sum())\n",
    "\n",
    "# Drop the 'message' column as it is completely null\n",
    "layer_3.drop(columns=['message'], inplace=True)\n",
    "layer_5.drop(columns=['message'], inplace=True)\n",
    "\n",
    "# Handle null values in 'value'\n",
    "if layer_3['value'].isnull().sum() > 0:\n",
    "    layer_3['value'].fillna(layer_3['value'].median(), inplace=True)\n",
    "\n",
    "if layer_5['value'].isnull().sum() > 0:\n",
    "    layer_5['value'].fillna(layer_5['value'].median(), inplace=True)\n",
    "\n",
    "print(\"\\nStatistical summary of layer 3:\")\n",
    "print(layer_3.describe(percentiles=[0.25, 0.5, 0.75]))\n",
    "print(\"\\nStatistical summary of layer 5:\")\n",
    "print(layer_5.describe(percentiles=[0.25, 0.5, 0.75]))\n",
    "\n",
    "# ---- 1.2. Distribution Analysis and Outlier Detection ----\n",
    "plt.figure(figsize=(10, 5))\n",
    "for col in layer_3.select_dtypes(include=np.number).columns:\n",
    "    plt.figure()\n",
    "    sns.histplot(layer_3[col], kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {col} in layer 3\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    sns.boxplot(y=layer_3[col])\n",
    "    plt.title(f\"Boxplot of {col} in layer 3\")\n",
    "    plt.show()\n",
    "\n",
    "    # Z-score for detecting outliers\n",
    "    layer_3[f\"{col}_zscore\"] = np.abs(zscore(layer_3[col]))\n",
    "    print(f\"Outliers detected in {col} (Z-score > 3) in layer 3:\")\n",
    "    print(layer_3[layer_3[f\"{col}_zscore\"] > 3][[col]])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for col in layer_5.select_dtypes(include=np.number).columns:\n",
    "    plt.figure()\n",
    "    sns.histplot(layer_5[col], kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {col} in layer 5\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    sns.boxplot(y=layer_5[col])\n",
    "    plt.title(f\"Boxplot of {col} in layer 5\")\n",
    "    plt.show()\n",
    "\n",
    "    # Z-score for detecting outliers\n",
    "    layer_5[f\"{col}_zscore\"] = np.abs(zscore(layer_5[col]))\n",
    "    print(f\"Outliers detected in {col} (Z-score > 3) in layer 5:\")\n",
    "    print(layer_5[layer_5[f\"{col}_zscore\"] > 3][[col]])\n",
    "\n",
    "# ---- 1.3. Temporal Analysis and Trends ----\n",
    "if \"user_ts\" in layer_3.columns:\n",
    "    layer_3[\"user_ts\"] = pd.to_datetime(layer_3[\"user_ts\"], format='%Y-%m-%d %H:%M:%S%z', errors='coerce')\n",
    "    layer_3 = layer_3.dropna(subset=['user_ts'])\n",
    "    layer_3 = layer_3.sort_values(by=\"user_ts\")\n",
    "    for col in layer_3.select_dtypes(include=np.number).columns:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(layer_3[\"user_ts\"], layer_3[col], label=col)\n",
    "        plt.title(f\"Time series of {col} in layer 3\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # Rolling Mean & Std\n",
    "    for col in layer_3.select_dtypes(include=np.number).columns:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(layer_3[\"user_ts\"], layer_3[col].rolling(window=30).mean(), label=\"Rolling Mean\")\n",
    "        plt.plot(layer_3[\"user_ts\"], layer_3[col].rolling(window=30).std(), label=\"Rolling Std\")\n",
    "        plt.title(f\"Rolling Mean & Std of {col} in layer 3\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "if \"user_ts\" in layer_5.columns:\n",
    "    layer_5[\"user_ts\"] = pd.to_datetime(layer_5[\"user_ts\"], format=\"mixed\", errors=\"coerce\")\n",
    "    layer_5 = layer_5.dropna(subset=['user_ts'])\n",
    "    layer_5 = layer_5.sort_values(by=\"user_ts\")\n",
    "    for col in layer_5.select_dtypes(include=np.number).columns:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(layer_5[\"user_ts\"], layer_5[col], label=col)\n",
    "        plt.title(f\"Time series of {col} in layer 5\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # Rolling Mean & Std\n",
    "    for col in layer_5.select_dtypes(include=np.number).columns:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(layer_5[\"user_ts\"], layer_5[col].rolling(window=30).mean(), label=\"Rolling Mean\")\n",
    "        plt.plot(layer_5[\"user_ts\"], layer_5[col].rolling(window=30).std(), label=\"Rolling Std\")\n",
    "        plt.title(f\"Rolling Mean & Std of {col} in layer 5\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# ---- 1.4. Model-Based Anomaly Detection ----\n",
    "X = layer_3.select_dtypes(include=np.number)  # Only numeric variables\n",
    "if not X.empty:\n",
    "\n",
    "    # Linear Regression\n",
    "    if X.shape[1] > 1:\n",
    "        reg = LinearRegression()\n",
    "        reg.fit(X.iloc[:, :-1], X.iloc[:, -1])\n",
    "        print(\"Linear regression coefficients for layer 3:\")\n",
    "        print(reg.coef_)\n",
    "\n",
    "    # DBSCAN for anomaly detection\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    layer_3[\"dbscan_anomaly\"] = dbscan.fit_predict(X)\n",
    "\n",
    "    # Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "    layer_3[\"isolation_anomaly\"] = iso_forest.fit_predict(X)\n",
    "\n",
    "    # LOF (Local Outlier Factor)\n",
    "    lof = LocalOutlierFactor(n_neighbors=20)\n",
    "    layer_3[\"lof_anomaly\"] = lof.fit_predict(X)\n",
    "\n",
    "    # ---- Visualize anomalies ----\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.scatterplot(x=X.iloc[:, 0], y=X.iloc[:, 1] if X.shape[1] > 1 else X.iloc[:, 0], hue=layer_3[\"isolation_anomaly\"], palette={1:\"blue\", -1:\"red\"})\n",
    "    plt.title(\"Anomalies detected with Isolation Forest in layer 3\")\n",
    "    plt.legend(title=\"Anomaly Layer 3\", labels=[\"Normal\", \"Anomalous\"])\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nNumber of anomalies detected by each method in layer 3:\")\n",
    "    print(\"Isolation Forest:\")\n",
    "    print(layer_3[\"isolation_anomaly\"].value_counts())\n",
    "    print(\"DBSCAN:\")\n",
    "    print(layer_3[\"dbscan_anomaly\"].value_counts())\n",
    "    print(\"Local Outlier Factor (LOF):\")\n",
    "    print(layer_3[\"lof_anomaly\"].value_counts())\n",
    "\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Select only numeric variables\n",
    "X = layer_5.select_dtypes(include=np.number)  # Only numeric variables\n",
    "\n",
    "# Reduce sample if dataset is too large\n",
    "sample_size = min(50000, len(X))  # Take a maximum of 50,000 rows\n",
    "X_sample = X.sample(n=sample_size, random_state=42)\n",
    "\n",
    "if not X_sample.empty:\n",
    "    # Linear Regression\n",
    "    if X_sample.shape[1] > 1:\n",
    "        reg = LinearRegression()\n",
    "        reg.fit(X_sample.iloc[:, :-1], X_sample.iloc[:, -1])\n",
    "        print(\"Linear regression coefficients for layer 5:\")\n",
    "        print(reg.coef_)\n",
    "\n",
    "    # MiniBatchKMeans for clustering detection\n",
    "    kmeans = MiniBatchKMeans(n_clusters=5, random_state=42, batch_size=10000)\n",
    "    layer_5.loc[X_sample.index, \"kmeans_cluster\"] = kmeans.fit_predict(X_sample)\n",
    "\n",
    "    # Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "    layer_5.loc[X_sample.index, \"isolation_anomaly\"] = iso_forest.fit_predict(X_sample)\n",
    "\n",
    "    # LOF (Local Outlier Factor)\n",
    "    lof = LocalOutlierFactor(n_neighbors=20)\n",
    "    layer_5.loc[X_sample.index, \"lof_anomaly\"] = lof.fit_predict(X_sample)\n",
    "\n",
    "    # ---- Visualize anomalies ----\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.scatterplot(x=X_sample.iloc[:, 0], y=X_sample.iloc[:, 1] if X_sample.shape[1] > 1 else X_sample.iloc[:, 0], hue=layer_5.loc[X_sample.index, \"isolation_anomaly\"], palette={1:\"blue\", -1:\"red\"})\n",
    "    plt.title(\"Anomalies detected with Isolation Forest in layer 5 (Reduced Sample)\")\n",
    "    plt.legend(title=\"Anomaly Layer 5\", labels=[\"Normal\", \"Anomalous\"])\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nNumber of anomalies detected by each method in layer 5 (Reduced Sample):\")\n",
    "    print(\"Isolation Forest:\")\n",
    "    print(layer_5.loc[X_sample.index, \"isolation_anomaly\"].value_counts())\n",
    "    print(\"MiniBatchKMeans Clusters:\")\n",
    "    print(layer_5.loc[X_sample.index, \"kmeans_cluster\"].value_counts())\n",
    "    print(\"Local Outlier Factor (LOF):\")\n",
    "    print(layer_5.loc[X_sample.index, \"lof_anomaly\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------Hypothesis1-----------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_TEMP_LAYER_1 = pd.read_csv(\"CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_1.csv\", index_col=0,parse_dates=True).drop(columns=[\"message\"], errors=\"ignore\").iloc[:57097]\n",
    "df_TEMP_LAYER_3 = pd.read_csv(\"CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_3.csv\", index_col=0,parse_dates=True).drop(columns=[\"message\"], errors=\"ignore\").iloc[:57097]\n",
    "df_TEMP_LAYER_5 = pd.read_csv(\"CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_5.csv\", index_col=0,parse_dates=True).drop(columns=[\"message\"], errors=\"ignore\").iloc[:57097]\n",
    "df_TEMP_LAYER_7 = pd.read_csv(\"CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_7.csv\", index_col=0,parse_dates=True).drop(columns=[\"message\"], errors=\"ignore\").iloc[:57097]\n",
    "df_TEMP_LAYER_9 = pd.read_csv(\"CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_9.csv\", index_col=0,parse_dates=True).drop(columns=[\"message\"], errors=\"ignore\").iloc[:57097]\n",
    "df_REJECTS = pd.read_csv(\"CONTIFORM_MMA_CONTIFORM_MMA1_WS_Tot_Rej_0.csv\", index_col=0,parse_dates=True).drop(columns=[\"message\"], errors=\"ignore\",inplace=True)\n",
    "\n",
    "df_REJECTS = pd.read_csv(\"CONTIFORM_MMA_CONTIFORM_MMA1_WS_Tot_Rej_0.csv\", index_col=0,parse_dates=True).drop(columns=[\"message\"], errors=\"ignore\")\n",
    "\n",
    "# Assign the DataFrame with dropped columns to df_REJECTS.\n",
    "df = df_REJECTS.copy()\n",
    "df[\"PreformTemperatureLayer_1\"] = df_TEMP_LAYER_1[\"value\"] # Select the 'value' column\n",
    "df[\"PreformTemperatureLayer_3\"] = df_TEMP_LAYER_3[\"value\"] # Select the 'value' column\n",
    "df[\"PreformTemperatureLayer_5\"] = df_TEMP_LAYER_5[\"value\"] # Select the 'value' column\n",
    "df[\"PreformTemperatureLayer_7\"] = df_TEMP_LAYER_7[\"value\"] # Select the 'value' column\n",
    "df[\"PreformTemperatureLayer_9\"] = df_TEMP_LAYER_9[\"value\"] # Select the 'value' column\n",
    "\n",
    "rejection_threshold = pd.to_numeric(df[df_REJECTS.columns[0]], errors='coerce').quantile(0.75)\n",
    "df[\"Rejection Group\"] = [\"High\" if x > rejection_threshold else \"Low\" for x in pd.to_numeric(df[df_REJECTS.columns[0]], errors='coerce')]\n",
    "\n",
    "for var in [\"PreformTemperatureLayer_1\", \"PreformTemperatureLayer_3\", \"PreformTemperatureLayer_5\", \"PreformTemperatureLayer_7\", \"PreformTemperatureLayer_9\"]:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.boxplot(x=\"Rejection Group\", y=var, data=df)\n",
    "    plt.title(f\"Distribution of {var} in Lots with High and Low Rejection\")\n",
    "    plt.xlabel(\"Rejection Group\")\n",
    "    plt.ylabel(\"Preform Temperature\")\n",
    "    plt.show()\n",
    "    \n",
    "#-----------------------------------------------------Hypothesis2------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "df_CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_1\n",
    "df_CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_3\n",
    "df_CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_5\n",
    "df_CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_7\n",
    "df_CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_9\n",
    "df_CONTIFORM_MMA_CONTIFORM_MMA1_WS_Tot_Rej_0\n",
    "'''\n",
    "\n",
    "\n",
    "#-------------------Attempting to see if bottles per day increased------------------\n",
    "\n",
    "dfs_a_graficar = {\n",
    "    \"Preform Temp Layer 1\": df_CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_1,\n",
    "    \"Preform Temp Layer 3\": df_CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_3,\n",
    "    \"Preform Temp Layer 5\": df_CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_5,\n",
    "    \"Preform Temp Layer 7\": df_CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_7,\n",
    "    \"Preform Temp Layer 9\": df_CONTIFORM_MMA_CONTIFORM_MMA1_PreformTemperatureLayer_9\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "\n",
    "fecha_inicio = pd.to_datetime(\"2024-11-15\").tz_localize(None)\n",
    "fecha_fin = pd.to_datetime(\"2025-12-01\").tz_localize(None)\n",
    "\n",
    "for nombre, df in dfs_a_graficar.items():\n",
    "    df = df.copy()  \n",
    "    df[\"user_ts\"] = pd.to_datetime(df[\"user_ts\"], errors=\"coerce\")\n",
    "    df[\"user_ts\"] = df[\"user_ts\"].dt.tz_localize(None)\n",
    "    df_filtrado = df[(df[\"user_ts\"] >= fecha_inicio) & (df[\"user_ts\"] <= fecha_fin)].sort_values(by=\"user_ts\")\n",
    "\n",
    "    if not df_filtrado.empty:\n",
    "        plt.plot(df_filtrado[\"user_ts\"], df_filtrado[\"value\"], label=nombre, alpha=0.7)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Fecha\")\n",
    "plt.ylabel(\"Valor\")\n",
    "plt.title(\"Serie de Tiempo de Temperaturas de Preforma y Total Rechazos (1 Nov - 2 Ene)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------Hypothesis3-----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# CSV file paths\n",
    "csv_files = {\n",
    "    \"Layer_1\": \"path_to_layer_1.csv\",\n",
    "    \"Layer_3\": \"path_to_layer_3.csv\",\n",
    "    \"Layer_5\": \"path_to_layer_5.csv\",\n",
    "    \"Layer_7\": \"path_to_layer_7.csv\",\n",
    "    \"Layer_9\": \"path_to_layer_9.csv\"\n",
    "}\n",
    "\n",
    "# Load each CSV and merge them into a single DataFrame by `user_ts`\n",
    "df_layers = []\n",
    "for layer, file in csv_files.items():\n",
    "    df_temp = pd.read_csv(file)\n",
    "    df_temp[\"user_ts\"] = pd.to_datetime(df_temp[\"user_ts\"])  # Convert timestamps\n",
    "    df_temp = df_temp.rename(columns={\"value\": layer})  # Rename the temperature column\n",
    "    df_layers.append(df_temp[[\"user_ts\", layer]])  # Keep only `user_ts` and temperature\n",
    "\n",
    "# Merge datasets by `user_ts`\n",
    "df = df_layers[0]\n",
    "for i in range(1, len(df_layers)):\n",
    "    df = pd.merge(df, df_layers[i], on=\"user_ts\", how=\"inner\")\n",
    "\n",
    "# Boxplots to visualize the temperature range in each layer\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df.drop(columns=[\"user_ts\"]))\n",
    "plt.title(\"Temperature Distribution Across Different Layers\")\n",
    "plt.ylabel(\"Temperature (°C)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Calculate key statistics\n",
    "stats = df.drop(columns=[\"user_ts\"]).describe(percentiles=[0.25, 0.5, 0.75])\n",
    "print(\"Temperature statistics by layer:\")\n",
    "print(stats)\n",
    "\n",
    "# Compare temperatures before blower replacement\n",
    "if \"blower_replacement\" in df.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for layer in csv_files.keys():\n",
    "        sns.lineplot(data=df, x=\"user_ts\", y=layer, label=layer, alpha=0.7)\n",
    "    plt.axvline(df[df[\"blower_replacement\"] == 1][\"user_ts\"].min(), color=\"red\", linestyle=\"dashed\", label=\"Replacement\")\n",
    "    plt.title(\"Temperatures Before Blower Replacement\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Temperature (°C)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Correlation between temperatures and maintenance frequency\n",
    "if \"maintenance\" in df.columns:\n",
    "    corr_matrix = df.drop(columns=[\"user_ts\"]).corr()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation Between Temperatures and Maintenance\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
